{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6bsoNvl6ps8",
        "outputId": "859f2b4a-c502-4c29-9a8f-f687122a8ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a parametric test, the mean square error is as follows: 0.24799942796951702\n",
            "Non-parametric test MSE: 0.24822439822091205\n",
            "What is the similarity between the predictions? False\n",
            "The value of C can be inverted.\n",
            "The Gaussian Process (MSE) should be tested as follows: 0.016701755550403177\n",
            "To classify multi-classes, SVC uses a one-to-one approach.\n",
            "No of SVM's or Support vectors : 50\n",
            "Did a support vector can be found in the 18th training sample? True\n",
            "The no of Support Vector's  class 2 from are : 21\n",
            "Calculation of the accuracy of the classification test is that : 0.9777777777777777\n"
          ]
        }
      ],
      "source": [
        "# First Task :\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import matplotlib.pyplot as plottt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as  numpie\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# A Function to Generate Data \n",
        "def toydata_generate(f, sizeofthesample, varation_of_the_noise):\n",
        "    x = numpie.linspace(0, 1, sizeofthesample)\n",
        "    t = f(x) + numpie.random.normal(scale=varation_of_the_noise, size=x.shape)\n",
        "    return x, t\n",
        "\n",
        "def f(x):\n",
        "    return  numpie.sin(2 * numpie.pi * x)\n",
        "\n",
        "# 1. Produce 100 points for testing and 10 points for training.\n",
        "train_for_x,train_for_y = toydata_generate(f, 10, 0.25)\n",
        "test_for_x = numpie.linspace(0, 1, 100)\n",
        "test_for_y = f(test_for_x)\n",
        "\n",
        "#2. Use the polynomial basis function (order M=9) in step two.\n",
        "poly_features = PolynomialFeatures(degree=9)\n",
        "Train_Phi_Feat = poly_features.fit_transform(train_for_x[:, numpie.newaxis])\n",
        "Test_Phi_Feat = poly_features.transform(test_for_x[:, numpie.newaxis])\n",
        "\n",
        "# 3. The model should be trained parametrically and the test MSE should be reported\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(Train_Phi_Feat,train_for_y)\n",
        "Pred_for_y = lr.predict(Test_Phi_Feat)\n",
        "Mean_Sqre_Err = mean_squared_error(test_for_y, Pred_for_y)\n",
        "print(\"In a parametric test, the mean square error is as follows:\", Mean_Sqre_Err)\n",
        "\n",
        "# 4.Predict non-parametrically\n",
        "K = Train_Phi_Feat @ Train_Phi_Feat.T\n",
        "k = Test_Phi_Feat @ Train_Phi_Feat.T\n",
        "non_paramertic_Pred_for_y= k @ numpie.linalg.inv(K) @train_for_y\n",
        "non_param_mean_sqaure_error = mean_squared_error(test_for_y,non_paramertic_Pred_for_y)\n",
        "print(\"Non-parametric test MSE:\", non_param_mean_sqaure_error)\n",
        "\n",
        "# 5. Compare the predictions\n",
        "print(\"What is the similarity between the predictions?\", numpie.allclose(Pred_for_y,non_paramertic_Pred_for_y))\n",
        "\n",
        "\n",
        "# Second Task :\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import numpy as numpie\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plottt\n",
        "\n",
        "#1. Using the RBF kernel, define the gram matrix\n",
        "gamma = 5\n",
        "K = rbf_kernel(X=train_for_x[:, numpie.newaxis], Y=train_for_x[:, numpie.newaxis], gamma=gamma)\n",
        "\n",
        "# Setting Up Beta = 10 for the covariance matrix\n",
        "beta = 10\n",
        "C = K + numpie.eye(len(K)) / beta\n",
        "\n",
        "#2. The invertibility of C should be checked\n",
        "try:\n",
        "    C_inv = numpie.linalg.inv(C)\n",
        "    print(\"The value of C can be inverted.\")\n",
        "except numpie.linalg.LinAlgError:\n",
        "    print(\"The value of C cannot be inverted.\")\n",
        "\n",
        "#3. All test samples should be calculated to determine the predictive mean\n",
        "Test_k = rbf_kernel(X=test_for_x[:, numpie.newaxis], Y=train_for_x[:, numpie.newaxis], gamma=gamma)\n",
        "Gauusian_Prediction_Pred_for_y = Test_k @ C_inv @train_for_y\n",
        "\n",
        "# Test MSE\n",
        "Gaussain_Process_Mean_Sqre_err = mean_squared_error(test_for_y, Gauusian_Prediction_Pred_for_y)\n",
        "print(\"The Gaussian Process (MSE) should be tested as follows:\", Gaussain_Process_Mean_Sqre_err)\n",
        "# ```\n",
        "\n",
        "# Third Task :\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Separate Datasets of training and Testing  from the iris _ dataset and load them up \n",
        "iris = load_iris()\n",
        "Train_for_X, Test_for_X,train_for_y, test_for_y = train_test_split(iris.data, iris.target, test_size=0.3, random_state=5)\n",
        "\n",
        "# SVC model creation and training\n",
        "model_svc = SVC()\n",
        "model_svc.fit(Train_for_X,train_for_y)\n",
        "\n",
        "# 2.1:Multi-class classification is handled by SVC by default using a one-versus-one approach\n",
        "print(\"To classify multi-classes, SVC uses a one-to-one approach.\")\n",
        "\n",
        "# 2.2: Number of SVM's or the support vectors\n",
        "print(\"No of SVM's or Support vectors :\", len(model_svc.support_))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2.3: Identifying  that the support vector is in the 18th training sample\n",
        "Is_Supprt_vectr = 18 in model_svc.support_\n",
        "print(\"Did a support vector can be found in the 18th training sample?\", Is_Supprt_vectr)\n",
        "\n",
        "# 2.4: Counting No Of SVM\"S from Class 2 \n",
        "Class_2_n_Support_Vectors = model_svc.n_support_[2]\n",
        "print(\"The no of Support Vector's  class 2 from are :\", Class_2_n_Support_Vectors)\n",
        "\n",
        "# 3: Calculate the accuracy of the classification test\n",
        "Pred_for_y = model_svc.predict(Test_for_X)\n",
        "test_accuracy = accuracy_score(test_for_y, Pred_for_y)\n",
        "print(\"Calculation of the accuracy of the classification test is that :\", test_accuracy)\n",
        "\n",
        "\n",
        "# I have completed the third task. SVM model classification test accuracy is reported by checking whether the 18th training sample is a support vector, calculating the number of support vectors from class 2, and calculating the number of support vectors from class 2.  .\n",
        "\n"
      ]
    }
  ]
}